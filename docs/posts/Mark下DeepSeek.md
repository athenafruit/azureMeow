---
title: Mark 下 DeepSeek
description: DeepSeek 发展历程、技术创新、行业影响及个人使用心得
source: https://mp.weixin.qq.com/s/UuE6Kv-tFujTh7bAd0xj_g
---

# Mark 下 DeepSeek

过年基本连续几天都在跟 DeepSeek 对话，包括但不限于讨论分析社会现象、学习、讨论书等。

**非常非常惊喜。**

昨天花了点时间补了下资讯及原理，饶是我这种从来不在公众号讨论工作相关的人，也要 Mark 一下留作记录。

> 世上方一日，AI 已千年。

---

## 1）DeepSeek 是什么

DeepSeek 是一家大模型公司，全称深度求索人工智能基础技术研究有限公司。成立于 2023 年 7 月 17 日，由知名私募巨头幻方量化孕育而生。

### 关键时间点

| 时间 | 事件 |
|------|------|
| 2024.01.05 | 发布 DeepSeek LLM（第一个大模型） |
| 2024.03.11 | 发布 DeepSeek-VL（视觉-语言模型），开源 |
| 2024.05.07 | 发布 DeepSeek-V2，打响中国大模型价格战，价格仅 GPT-4o 的 2.7% |
| 2024.12.26 | 发布并开源 DeepSeek-V3，**500 万美金 + 2048 块 H800 GPU** 完成 6710 亿参数模型训练 |
| 2025.01.20 | 发布并开源 DeepSeek-R1，效果媲美 OpenAI o1，API 价格仅 o1 的 3.7% |
| 2025.01.28 | 发布并开源视觉多模态大模型 Janus-Pro 和 JanusFlow |

同时在数据上，上线 19 天日活 1909 万，霸榜全球。

![DeepSeek 数据](/images/wechat/Mark下DeepSeek_5c8cf9d6875e.jpg)

---

## 2）DeepSeek 为什么会火

直接借鉴游戏科学创始人、CEO，《黑神话：悟空》制作人冯骥的话：

> "DeepSeek，可能是个国运级别的科技成果。"

![冯骥评价](/images/wechat/Mark下DeepSeek_f02d24a4ba03.jpg)

### 结果上的体现

用更低廉的成本训练出了性能不错的模型。

### 更在于技术创新

- DeepSeek 这次最亮眼的是证明了**纯粹的 RL 能够直接把模型提到 o1 水平**，在此之前，业内所有人都认为需要 PRM（Process Reward Model）才能做到这点，这已经是颠覆行业的发现了
- 另外非常重要的是 DeepSeek 还发现了这种训练方式甚至能够让模型自己学会 longer-chain reasoning 以及 reflection，他们所谓"aha moment"
- 相当于只训练 LLM 得到更准确的结果，LLM 就能自己学会反思，思考到一半知道自己这样做下去会错，然后尝试自己纠错，这种模型"自我进化"的特性是业内仅次于 GPT intelligence emergence 的重大发现

就结果而言，"用更少的卡训练出效果差不多的模型"可能不仅仅是节约成本这么简单，更是一种 **improvement of scaling law**，意味着这种方法往上堆更多的卡有可能把模型能力再往上提升一个数量级，甚至直接达到 AGI。

---

## 3）DeepSeek 怎么训练出来的

### 3.1 训练过程

DeepSeek 团队做的第一件事，就是尝试"零监督"直接对基础模型进行大规模强化学习训练（即纯 RL），得到了 DeepSeek-R1-Zero。但 DeepSeek-R1-Zero 的这种"自发行为"有时也带来缺点，比如文字可读性差、语言混乱等。

**为了解决这一问题，他们设计了一个四阶段的流程，让模型从"能思考"到"会表达"**，DeepSeek-R1 也就此诞生：

1. 先收集少量的高质量长链式推理数据（Long Chain-of-Thought），让模型在上面做一个初步的监督微调（SFT）作为冷启动
2. 接着使用类似 DeepSeek-R1-Zero 的强化学习方法训练模型
3. 得到通过 RL 训练后模型产出的较大规模推理数据和通用 SFT 数据后，通过"拒绝采样（Rejection Sampling）"的方法训练和微调 DeepSeek-V3 这一基座模型
4. 最后再整体进行一次"全场景强化学习（Reinforcement Learning for all Scenarios）"，最终得到了 DeepSeek R1

### 3.2 训练背后是怎样的一群人

**人物画像：天才年轻人**

"都是一些 Top 高校的应届毕业生、没毕业的博四、博五实习生，还有一些毕业才几年的年轻人。"没有工作履历，DeepSeek 衡量年轻毕业生"优秀"与否的标准，除了院校，还有竞赛成绩。基本都是 ACM 金奖、本科就发多篇顶会论文的神人。

**如何管理：砸钱、给卡、扁平和学院派的管理方式**

只要技术提案有潜力，给人才的算力不限，此外钱也是给到位的，不输字节"往上加价"。另一方面，DeepSeek 每个成员不带团队，而是根据具体的目标，分成不同的研究小组。组内成员之间没有固定分工和上下级关系，"每个人都负责自己最擅长解决的部分，遇到困难就一起讨论，或者向其他组的专家讨教。"

---

## 4）DeepSeek 有什么影响

### 1. 海外慌了

各家轮番上阵研究且发言，而且感谢 DeepSeek，OpenAI 的 o3-mini 发布且免费了。

### 2. 冲击英伟达

R1 模型通过重新设计训练流程，**在保持高准确性的同时显著降低了内存占用和计算开销**，仅用了少量的低端 GPU（以 A100 为主）就实现了高端 GPU（以 H100 为代表）才有的性能。

但长期来看应该不会颠覆英伟达：

> - **高端芯片的统治力**：预计 2025 年英伟达从 Blackwell 架构产品线获得的收入有可能会超过市场的预期
> - **CUDA 生态壁垒**：90% 的 AI 开发者依赖 CUDA 平台，迁移成本极高
> - **供应链控制**：台积电 CoWoS 产能优先分配英伟达，2025 年预计英伟达占据 CoWoS 总需求的 63%

### 3. 长期来看，对 AI 模型厂商格局影响

- **底座模型**由几个超级公司来做，更大更好，开源
- **垂类模型**各行业有丰富的行业数据，可以在底座模型之上做更各领域的垂类模型
- **定制化**具体到各个业务再依托落地场景，做定制化的微调和工程落地

应用层的全栈是趋势：懂需求懂落地（简单的设计与前后端）懂营销。

---

## 5）可以做什么

### 1. 强烈推荐使用起来

已经连续一周每天在使用 DeepSeek 了，此前对于 Kimi 也没有这么重度的使用——大多数情况下，Kimi 能帮助总结替代搜索，简单的生活和工作场景已经完全由 Kimi 覆盖，包括好奇心搜索、知识补充、读药品说明书、翻译、润色总结、甚至讨论产品方案等。

### 2. 推理过程

我个人非常 enjoy R1 的思考推理过程，看它的思考过程比看最后的答案输出更有趣——你可以关注到它是怎么想的，本质上是思维链的学习。

整体的思考过程是一个产品需要去打磨的职业思维方式——先看用户的问题是什么，再看用户的需求是什么（为什么会有这个问题，希望得到什么），再看解决方案是什么，如何把解决方案更好的呈现给用户——妥妥的以用户为中心。

### 3. 使用技术

如上所述，用起来。某些时候觉得达到 60 分勉强可用的领域，随着模型能力的提升，未来一定会产生质变，典型场景如 Cursor 等，在 23 年 5 月算用着还可以，在 Claude 3.5 在编程领域碾压后，Cursor 能力有了巨大提升。

使用最新的技术，一开始会有一些上手门槛，但随着技术的普及和成本的降低，最终上手门槛会降低，成为生活中密不可分的事物——一如蒸汽、电、互联网。

### 4. 多提问

是对自己的要求。大语言模型囊括了人类的经验，O1 等推理模型学会了思维方式（就 R1 所体现的思考过程，其逻辑性和完善程度绝对超过 99% 的人）。

> **而提问，问为什么，这一刻的动心起念，AI 还有漫长的路要走。**
