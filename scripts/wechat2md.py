#!/usr/bin/env python3
"""
å…¬ä¼—å·æ–‡ç« æ‰¹é‡è½¬æ¢ä¸º Markdown
ä½¿ç”¨æ–¹æ³•ï¼š
1. æŠŠå…¬ä¼—å·æ–‡ç« é“¾æ¥æ”¾åˆ° articles.txtï¼Œæ¯è¡Œä¸€ä¸ª
2. è¿è¡Œï¼špython scripts/wechat2md.py
3. è½¬æ¢åçš„æ–‡ç« ä¼šä¿å­˜åˆ° docs/posts/ ç›®å½•
"""

import os
import re
import sys
import time
import hashlib
import requests
from pathlib import Path
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
import html2text

# é…ç½®
OUTPUT_DIR = Path(__file__).parent.parent / "docs" / "posts"
IMAGES_DIR = Path(__file__).parent.parent / "docs" / "public" / "images" / "wechat"
ARTICLES_FILE = Path(__file__).parent / "articles.txt"

# è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
}


def setup_dirs():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    IMAGES_DIR.mkdir(parents=True, exist_ok=True)


def fetch_article(url: str) -> str:
    """è·å–æ–‡ç«  HTML å†…å®¹"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        response.encoding = 'utf-8'
        return response.text
    except Exception as e:
        print(f"  âŒ è·å–æ–‡ç« å¤±è´¥: {e}")
        return None


def download_image(img_url: str, article_slug: str) -> str:
    """ä¸‹è½½å›¾ç‰‡å¹¶è¿”å›æœ¬åœ°è·¯å¾„"""
    try:
        # å¤„ç†ç›¸å¯¹è·¯å¾„
        if img_url.startswith('//'):
            img_url = 'https:' + img_url
        
        # è·³è¿‡ data URL
        if img_url.startswith('data:'):
            return img_url
        
        # ç”Ÿæˆæ–‡ä»¶å
        ext = os.path.splitext(urlparse(img_url).path)[1] or '.jpg'
        if ext not in ['.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg']:
            ext = '.jpg'
        
        # ä½¿ç”¨ URL çš„ hash ä½œä¸ºæ–‡ä»¶å
        img_hash = hashlib.md5(img_url.encode()).hexdigest()[:12]
        filename = f"{article_slug}_{img_hash}{ext}"
        filepath = IMAGES_DIR / filename
        
        # å¦‚æœå·²å­˜åœ¨åˆ™è·³è¿‡
        if filepath.exists():
            return f"/images/wechat/{filename}"
        
        # ä¸‹è½½å›¾ç‰‡
        response = requests.get(img_url, headers=HEADERS, timeout=30)
        response.raise_for_status()
        
        with open(filepath, 'wb') as f:
            f.write(response.content)
        
        return f"/images/wechat/{filename}"
    
    except Exception as e:
        print(f"    âš ï¸ å›¾ç‰‡ä¸‹è½½å¤±è´¥: {img_url[:50]}... ({e})")
        return img_url


def parse_wechat_article(html: str, url: str) -> dict:
    """è§£æå…¬ä¼—å·æ–‡ç« """
    soup = BeautifulSoup(html, 'html.parser')
    
    # è·å–æ ‡é¢˜
    title_tag = soup.find('h1', class_='rich_media_title') or soup.find('h1')
    title = title_tag.get_text(strip=True) if title_tag else "æœªå‘½åæ–‡ç« "
    
    # è·å–ä½œè€…/å…¬ä¼—å·å
    author_tag = soup.find('a', id='js_name') or soup.find('span', class_='rich_media_meta_nickname')
    author = author_tag.get_text(strip=True) if author_tag else ""
    
    # è·å–å‘å¸ƒæ—¶é—´
    time_tag = soup.find('em', id='publish_time') or soup.find('span', id='publish_time')
    pub_time = time_tag.get_text(strip=True) if time_tag else ""
    
    # è·å–æ­£æ–‡å†…å®¹
    content_tag = soup.find('div', id='js_content') or soup.find('div', class_='rich_media_content')
    
    if not content_tag:
        return None
    
    # ç”Ÿæˆæ–‡ç«  slugï¼ˆç”¨äºå›¾ç‰‡å‘½åï¼‰
    slug = re.sub(r'[^\w\u4e00-\u9fff]', '_', title)[:30]
    
    # å¤„ç†å›¾ç‰‡
    for img in content_tag.find_all('img'):
        # å…¬ä¼—å·å›¾ç‰‡å¯èƒ½åœ¨ data-src å±æ€§
        img_url = img.get('data-src') or img.get('src')
        if img_url:
            local_path = download_image(img_url, slug)
            img['src'] = local_path
            # ç§»é™¤å…¶ä»–å±æ€§
            for attr in list(img.attrs.keys()):
                if attr not in ['src', 'alt']:
                    del img[attr]
    
    # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
    for tag in content_tag.find_all(['script', 'style', 'iframe']):
        tag.decompose()
    
    # è½¬æ¢ä¸º Markdown
    h2t = html2text.HTML2Text()
    h2t.ignore_links = False
    h2t.ignore_images = False
    h2t.ignore_emphasis = False
    h2t.body_width = 0  # ä¸æ¢è¡Œ
    h2t.unicode_snob = True
    
    markdown = h2t.handle(str(content_tag))
    
    # æ¸…ç†å¤šä½™ç©ºè¡Œ
    markdown = re.sub(r'\n{3,}', '\n\n', markdown)
    
    return {
        'title': title,
        'author': author,
        'pub_time': pub_time,
        'content': markdown,
        'slug': slug,
        'url': url
    }


def save_article(article: dict) -> str:
    """ä¿å­˜æ–‡ç« ä¸º Markdown æ–‡ä»¶"""
    # ç”Ÿæˆæ–‡ä»¶å
    filename = re.sub(r'[^\w\u4e00-\u9fff\-]', '_', article['title'])
    filename = re.sub(r'_+', '_', filename).strip('_')
    filepath = OUTPUT_DIR / f"{filename}.md"
    
    # é¿å…é‡å
    counter = 1
    while filepath.exists():
        filepath = OUTPUT_DIR / f"{filename}_{counter}.md"
        counter += 1
    
    # ç”Ÿæˆ frontmatter
    frontmatter = f"""---
title: {article['title']}
description: {article['author']} çš„å…¬ä¼—å·æ–‡ç« 
source: {article['url']}
---

"""
    
    # å†™å…¥æ–‡ä»¶
    content = frontmatter + f"# {article['title']}\n\n" + article['content']
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return filepath.name


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("ğŸ“ å…¬ä¼—å·æ–‡ç« æ‰¹é‡è½¬æ¢å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥æ–‡ç« åˆ—è¡¨æ–‡ä»¶
    if not ARTICLES_FILE.exists():
        print(f"\nâš ï¸ æœªæ‰¾åˆ°æ–‡ç« åˆ—è¡¨æ–‡ä»¶: {ARTICLES_FILE}")
        print(f"\nè¯·åˆ›å»º {ARTICLES_FILE} æ–‡ä»¶ï¼Œæ¯è¡Œæ”¾ä¸€ä¸ªå…¬ä¼—å·æ–‡ç« é“¾æ¥")
        print("\nç¤ºä¾‹ï¼š")
        print("https://mp.weixin.qq.com/s/xxxxx")
        print("https://mp.weixin.qq.com/s/yyyyy")
        
        # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
        with open(ARTICLES_FILE, 'w', encoding='utf-8') as f:
            f.write("# æŠŠå…¬ä¼—å·æ–‡ç« é“¾æ¥æ”¾åœ¨ä¸‹é¢ï¼Œæ¯è¡Œä¸€ä¸ª\n")
            f.write("# ä»¥ # å¼€å¤´çš„è¡Œä¼šè¢«å¿½ç•¥\n")
            f.write("# https://mp.weixin.qq.com/s/xxxxx\n")
        
        print(f"\nâœ… å·²åˆ›å»ºç¤ºä¾‹æ–‡ä»¶: {ARTICLES_FILE}")
        return
    
    # è¯»å–æ–‡ç« é“¾æ¥
    with open(ARTICLES_FILE, 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
    
    if not urls:
        print("\nâš ï¸ æ–‡ç« åˆ—è¡¨ä¸ºç©ºï¼Œè¯·æ·»åŠ å…¬ä¼—å·æ–‡ç« é“¾æ¥")
        return
    
    print(f"\nğŸ“‹ å…±æ‰¾åˆ° {len(urls)} ç¯‡æ–‡ç« å¾…è½¬æ¢\n")
    
    setup_dirs()
    
    success_count = 0
    failed_urls = []
    
    for i, url in enumerate(urls, 1):
        print(f"[{i}/{len(urls)}] å¤„ç†: {url[:60]}...")
        
        # è·å–æ–‡ç« 
        html = fetch_article(url)
        if not html:
            failed_urls.append(url)
            continue
        
        # è§£ææ–‡ç« 
        article = parse_wechat_article(html, url)
        if not article:
            print(f"  âŒ è§£æå¤±è´¥")
            failed_urls.append(url)
            continue
        
        # ä¿å­˜æ–‡ç« 
        filename = save_article(article)
        print(f"  âœ… å·²ä¿å­˜: {filename}")
        success_count += 1
        
        # é¿å…è¯·æ±‚è¿‡å¿«
        if i < len(urls):
            time.sleep(1)
    
    # æ±‡æ€»
    print("\n" + "=" * 50)
    print(f"âœ… æˆåŠŸè½¬æ¢: {success_count} ç¯‡")
    if failed_urls:
        print(f"âŒ å¤±è´¥: {len(failed_urls)} ç¯‡")
        print("\nå¤±è´¥çš„é“¾æ¥ï¼š")
        for url in failed_urls:
            print(f"  - {url}")
    
    print(f"\nğŸ“ æ–‡ç« ä¿å­˜ä½ç½®: {OUTPUT_DIR}")
    print(f"ğŸ“ å›¾ç‰‡ä¿å­˜ä½ç½®: {IMAGES_DIR}")
    
    # æ¸…ç©ºå·²å¤„ç†çš„é“¾æ¥ï¼ˆä¿ç•™è¯´æ˜æ³¨é‡Šï¼‰
    if success_count > 0:
        with open(ARTICLES_FILE, 'w', encoding='utf-8') as f:
            f.write("# æŠŠå…¬ä¼—å·æ–‡ç« é“¾æ¥æ”¾åœ¨ä¸‹é¢ï¼Œæ¯è¡Œä¸€ä¸ª\n")
            f.write("# ä»¥ # å¼€å¤´çš„è¡Œä¼šè¢«å¿½ç•¥\n")
            f.write("# è½¬æ¢å®Œæˆåä¼šè‡ªåŠ¨æ¸…ç©ºæ­¤æ–‡ä»¶\n\n")
            # ä¿ç•™å¤±è´¥çš„é“¾æ¥ï¼Œæ–¹ä¾¿é‡è¯•
            if failed_urls:
                f.write("# ä»¥ä¸‹æ˜¯è½¬æ¢å¤±è´¥çš„é“¾æ¥ï¼Œå¯ä»¥é‡è¯•ï¼š\n")
                for url in failed_urls:
                    f.write(f"# {url}\n")
        print("\nğŸ§¹ å·²æ¸…ç©ºé“¾æ¥åˆ—è¡¨")
    
    print("\nğŸ’¡ æç¤ºï¼šè½¬æ¢å®Œæˆåï¼Œè®°å¾—æ›´æ–° config.mts çš„ sidebar é…ç½®")


if __name__ == '__main__':
    main()

